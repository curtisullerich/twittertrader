%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{hyperref}

\nocopyright
\frenchspacing
\pdfinfo{
/Title (Classification of Tweets by Company)
/Author (Curtis Ullerich, Daniel Stiner, Brandon Maxwell)}
\setcounter{secnumdepth}{0}  
\begin{document}

\title{Tweet Classification - Filtering \\ of Twitter for Company-relevant Tweets}
\author{
Curtis Ullerich, Daniel Stiner, Brandon Maxwell\\
Department of Computer Science and Engineering\\
Iowa State University
Ames, Iowa, USA\\
\{curtisu,stiner,bmaxwell\}@iastate.edu\\
}
\maketitle
\begin{abstract}
\begin{quote}
Twitter is a popular source for data mining due to its massive scale and inclusivity of current trends. As businesses seek to discover public opinion about their business or products, Twitter is one source for mining this information. Tweets present interesting classification challenges due to their irregular formatting and relatively small number of features. Tweet selection by keyword filtering often suffers from an high accuracy and low recall, or low accuracy and high recall. Using machine learning for text classification can improve this. We aim to survey a variety of text processing steps, focusing on the effects of tokenization, to increase accuracy of classification using a Naive Bayes classifier. We use a broad set of keywords with the goal of collecting a superset of the Tweets about a particular company (Apple, in this case) and train a binary classifier to filter out false positives.
\end{quote}
\end{abstract}

\section{Introduction}

\subsection{Goals}

\begin{itemize}

\item Collect a corpus that represents a superset of tweets about Apple and label these instances as "apple" or "none"
\item Create preprocessing steps to maximize the accuracy of binary classification

\end{itemize}

\subsection{Hypotheses}
\item Using URL-replacement will improve disambiguation accuracy. 
\item adding features for emoticon classes....
\item adding features for @references, retweets, and hashtags, will decrease the feature set while improving accuracy
\item selection of a proper tokenization regex will have profound effects on accuracy
\item Stopping and stemming....

\section{Related Work}
DUALIST is a system that extends Mallet by user-interactive feedback during training[Settles]. Included in their implementation was a TwitterPipe that performed some simple feature extraction. 
Part of the Sentiment Analysis Symposium focuses on tokenization of tweets for sentiment analysis, demonstrating that it provides consistent improvement over whitespace-based tokenizing \cite{potts2011}. A good deal of research has been done into sentiment classification of tweets \cite{Pak10}

\section{System Breakdown}
\textbf{Corpus creation}\\
By accessing the Twitter "Firehose" stream through their public API, we harvested tens of thousands of tweets using broad keyword filters with the goal of collecting a superset of relevant tweets. This provides us a smaller set of features in the overal corpus. The keywords used for the company of choice during this round of testing, Apple, were collected from information on the company's website and the Wikipedia page about Apple:\\
...\\
\\
This set of keywords clearly allows in a large number of false positives:\\
nice fish and chip.. good pie apple :-) http://t.co/KVDN5r5l \\
20 chicken nuggets, chips, big mac, and a vanilla milkshake LIFE IS GOOD \\
1 more day whoop plus safari ride yay \\
shley staples an apple.  "AHHHH!!!!  Apple juice in my eye!" \\
I ain't seen uncle Mac in a while\\
\\
During the time period of our data collection (give time period), the ratio was 40\% topical and 60\% false positives [need to analyze this for sure].
On Twitter, a single message posted is often reposted, or "re-tweeted," many more times in a very short timespan. To reduce the effects of temporal overfitting, in which our model would overfit the data for a particular time slice, we filtered the data to remove retweets and near-duplicates. Tweets automatically osted by applications, iPad games, for instance, are often near-duplicates, so we removed these from the data set as well. This reduced a collection of 100,000 tweets by 50\%. \\
A computer-posted tweet: I've collected 10,650 gold coins! http://t.co/H0V0O6yP #ipad, #ipadgames, #gameinsight\\
A retweet, signified by "RT": RT @PHILerNotebook: Trying to fix my Mac. This gray loading bar takes 10 minutes to start up. Grr. Shall take it to an _Apple_ _store_ soon! \\
\\
We hand labeled 2000 tweets for use in training and testing, including only English-language tweets in our training set. \\
Data sets and scripts used for processing text data are available on the project website.

\textbf{Data aggregation and classification}\\

\subsection{Basic Approach}

We do all machine learning with the Mallet[2] library developed by the University of Massachusetts. Mallet includes abstractions for a data processing pipeline, during which we use pre-built and custom processing "Pipes" to transform the data, beginning in our case with raw tweets text and ending in Mallet's internal FeatureVector format. We built several Pipes to analyze the effect of different filters and processes on the accuracy of classification:
SpellCheck\\

Stemmer\\

Tokenizer\\
Twitter data, and user-generated web content in general, provides unique challenges to an automatic tokenizing system due to the relatively small number of features present and high level of noise in the feature set [citation and more details]. Features on twitter include excessive and irregular punctuation, frequent misspellings, emoticons, URLs, emoticons, and hashtags, to name a few.

Cool ranch , 4 berry sundae , apple juice #yessuh #latenight #snack http://t.co/ttc3vujp \\
I just poured apple juice on my cereal.. Gah. #SoTired \\
I hate Siri.. -___- \\

We approach this problem using a layered feature extraction system using regular expressions. Some features will often contain ambiguous or incorrect matches to other tokens. A prime example of this is the emoticon :/, which appears in every http link: http:/t.co/p9weubf [bold the emoticon somehow]. To alleviate such problems, we begin by tokenizing in order: URLs, emoticons, usernames, and hashtags. At this point, we are left with a fragmented tweet containing myriad punctuation, capitalization, and creative spellings. As done by Potts (2011) we normalize the length of all letter sequences greater than two, as in English, these are invariably due to user-added emphasis, and condensing the larger set of tokens focuses the feature set.\\

#news can't wait for my signed #OoRITE2OutNow it's emotional here right now loooool #OoRITE2OutNow https://t.co/r4n4cn6C \\

Capitalization\\
As presented in our results, because of irregular capitalization, two tokens likely to be semantically identical with respect to company classification may be represented in multiple cases: {Nooo, NOOO, nooo, NoOo}. To alleviate this, we attempt three different approaches. In the first, we simply lowercase all remaining text (note that many URLs become invalid after modifying case). In the second, we lowercase any text that is not in all caps. This serves to retain acronyms. In the third, we lowercase everything, leaving the case of all strings containing "apple" (case insensitive) alone. We made this decision based on the semantic difference between the capitalization of Apple the company and apple the fruit. This can be generalized to a larger set of keywords and applied to other companies in a similar fashion. 

Miscenalaneous\\


Unescape HTML\\

N-grams\\

Link2Title\\

Stopword Removal\\


\section{Results}

Discussion of accuracy etc

\section{Applications and Future Work}

Things that could be done....

\section{Contributions}

We have built several reusable Java components that extend the Mallet API for processig of data from Twitter. We created a TweetJsonIterator that accepts Twitter's JSON-formatted tweets and creates Mallet Instances containing their values for easy processing by Mallet's pipeline during model training. We have also created several Pipe implementations that can be reused or easily modified to suit a similar Twitter processing. These include FixEmoticons, Link2Title, Stemmer, SpellCheck, and TwitterFeatures. All code is available through our project website.

\section{ Acknowledgments}

\begin{thebibliography}{99}
@article{potts2011,
    author    = "Christopher Potts",
    title     = "Sentiment Symposium Tutorial: Tokenizing",
    journal   = "",
    year      = "2011"
}
@inproceedings{Pak10,
  added-at = {2010-06-01T10:23:44.000+0200},
  address = {Valletta, Malta},
  author = {Pak, Alexander and Paroubek, Patrick},
  biburl = {http://www.bibsonomy.org/bibtex/25656c3bb1adf00c58a85e3204096961c/frederik},
  booktitle = {Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC'10)},
  date = {19-21},
  interhash = {ac930b0459a3c8a2fc2d74c52a475026},
  intrahash = {5656c3bb1adf00c58a85e3204096961c},
  isbn = {2-9517408-6-7},
  keywords = {imported},
  language = {english},
  month = may,
  publisher = {European Language Resources Association (ELRA)},
  timestamp = {2010-06-01T10:23:44.000+0200},
  title = {Twitter as a Corpus for Sentiment Analysis and Opinion Mining},
  url = {http://www.lrec-conf.org/proceedings/lrec2010/pdf/385_Paper.pdf},
  year = 2010
}
\end{thebibliography}

[1] J. Bollen, H. Mao, and X.-J. Zeng. Twitter mood predicts the
stock market. Journal of Computational Science,
abs/1010.3003, 2010\\

[2] Mallett, University of Massachussetts Department of Computer Science\\

[3] Sanders Analytics (http://www.sananalytics.com/lab/twitter-sentiment/)

\end{document}
